{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gu0y1/CS3244/blob/main/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a855393",
      "metadata": {
        "id": "9a855393"
      },
      "source": [
        "Available at [Canvas](https://https://www.nus.edu.sg/canvas/) \"CS3244/Assignments/Assignment 1 (Individual)\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbbae3e4-d223-47a6-9825-f548841c5a57",
      "metadata": {
        "id": "fbbae3e4-d223-47a6-9825-f548841c5a57"
      },
      "source": [
        "---\n",
        "\n",
        "**Make sure you reserve sufficient time to upload your exported copy to Canvas.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TZIaecT9uF6r",
      "metadata": {
        "id": "TZIaecT9uF6r"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "We have introduced $k$**-Nearest Neighbors** and **Decision Trees** in the lectures on Weeks 02 and 03. In this **individual** assignment, you will be graded on implementing the models yourself. You will run both models and compare their performance on the same dataset. Overall, you will learn to appreciate the similarities and differences in models to predict on the same problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hFe14MROoltp",
      "metadata": {
        "id": "hFe14MROoltp"
      },
      "source": [
        "## Assignment Instructions\n",
        "\n",
        "Before the assignment, you should create a copy of this Colab file in your own Google Drive.\n",
        "\n",
        "In this assignment, you will\n",
        "\n",
        "1. Follow the instructions to familiarize yourself with the `wine` dataset;\n",
        "2. Write your code in the designated spaces to finish the implementations of the $k$-Nearest Neighbor and Decision Tree algorithms;\n",
        "2. Run the notebook to obtain your results;\n",
        "3. Upload this Colab file into Canvas \"CS3244/Assignments/Assignment 1 (Individual)\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a75b5185-8cb4-49c3-978f-7c234bee90b8",
      "metadata": {
        "id": "a75b5185-8cb4-49c3-978f-7c234bee90b8"
      },
      "source": [
        "## 1. Programming: $k$-NN and Decision Tree from `sklearn`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24e5f97a-beff-4773-b90c-55ec53eec6e0",
      "metadata": {
        "id": "24e5f97a-beff-4773-b90c-55ec53eec6e0"
      },
      "source": [
        "### .a Loading and Visualizing Input data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59fcb9eb-aa7d-43db-9378-8af8983eec88",
      "metadata": {
        "id": "59fcb9eb-aa7d-43db-9378-8af8983eec88"
      },
      "source": [
        "\n",
        "We'll use the [Wine](https://archive.ics.uci.edu/ml/datasets/Wine) dataset from the popular [UCI dataset repository](https://archive.ics.uci.edu/ml/index.php).  This describes the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 chemical constituents found in each of the three types of wines.\n",
        "\n",
        "We'll load in the data for white wines, take a look around and split the data into parts for training the model and testing it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "ebed0d46-50bd-447d-9292-256bfacab8cd",
      "metadata": {
        "id": "ebed0d46-50bd-447d-9292-256bfacab8cd"
      },
      "outputs": [],
      "source": [
        "# Import the standard tools for pythonic data analysis.\n",
        "import csv\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Let's read the data in as a \"data frame\" (df), equivalent to our D = (X,y) data matrix\n",
        "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv',sep=';') # Separate on semicolons"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6af47084-cf5c-44f7-9c7a-dc7b59b0decb",
      "metadata": {
        "id": "6af47084-cf5c-44f7-9c7a-dc7b59b0decb"
      },
      "source": [
        "We want to apply matchine learning algorithms to predict the quality of the wine from its constituents. That is, we will utilize algorithms that can find the correlation between wine quality and its 13 constituent (as features) to make prediction. To better understand how algorithms can achieve this, we need to inspect the data distribution first.\n",
        "<!-- (actually, we can predict any feature from any other feature, there's really no particular distinction between $\\mathbf{x}$ and $y$).   -->\n",
        "Let's take a look at the distribution for **quality**. This tells us the how the wine quality is distributed without knowing anything about other information. Go ahead and run the below cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "778831d9-de53-4da4-8ade-e3dcbd2922b8",
      "metadata": {
        "id": "778831d9-de53-4da4-8ade-e3dcbd2922b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        },
        "outputId": "e7777e44-6353-4400-c142-48c28dfe32be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of wines of a particular rating:\n",
            "6    2198\n",
            "5    1457\n",
            "7     880\n",
            "8     175\n",
            "4     163\n",
            "3      20\n",
            "9       5\n",
            "Name: quality, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<Axes: title={'center': 'quality'}>]], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Get numeric distributions of our quality output\n",
        "print(\"Number of wines of a particular rating:\")\n",
        "counts = df['quality'].value_counts()\n",
        "print(counts)\n",
        "\n",
        "# Let's do a histogram plot. To do that we need to specify the\n",
        "# y-axis that we want to plot – i.e. 'quality' – and number of bins\n",
        "df.hist(column = 'quality', bins = len(counts))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1904d12-8911-4ea4-a4dc-5cb3157a0fd1",
      "metadata": {
        "id": "c1904d12-8911-4ea4-a4dc-5cb3157a0fd1"
      },
      "source": [
        "This looks somewhat normally distributed.  Let's say we care about good wines.  Then we'll just concentrate on differentiating great wines from the rest. Our task is now a classification. As we can see from the histogram below, the two classes are heavily imbalanced -- we have far more \"Not Good\" wines than the \"Good\" wines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "9aebf57c-d97c-469b-9bd5-68f660902706",
      "metadata": {
        "id": "9aebf57c-d97c-469b-9bd5-68f660902706",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "outputId": "10e8c513-6f81-46f9-e89c-709aef66ef59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: title={'center': 'Distribution of classes'}>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Create a new column called good wine, where the value of the quality is 7 or better.\n",
        "df['good wine'] = np.where(df['quality']>=7, \"Good\", \"Not Good\")\n",
        "\n",
        "# Then remove the quality column (why)\n",
        "df.drop('quality', axis=1, inplace=True)\n",
        "\n",
        "df['good wine'].value_counts().plot(kind = 'bar', title = 'Distribution of classes')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e34552f-c406-440a-a3fa-86e9337e7013",
      "metadata": {
        "id": "2e34552f-c406-440a-a3fa-86e9337e7013"
      },
      "source": [
        "### .b Split dataset into train and test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd2e9a7f-21ce-461a-884e-519bfb045f8a",
      "metadata": {
        "id": "fd2e9a7f-21ce-461a-884e-519bfb045f8a"
      },
      "source": [
        "Let's split the data into training and testing data sets before we do anything else.   It's important to look only at the training data to develop our intuitions (**why?**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "1d8492c0-aea9-44ce-8f74-38bd2f24b038",
      "metadata": {
        "id": "1d8492c0-aea9-44ce-8f74-38bd2f24b038",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ae29644-1e57-4256-e853-d4ed66709f8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training instances:  3428 \n",
            "Number of test instances:  1470\n"
          ]
        }
      ],
      "source": [
        "# Learn how to split test and training data from a whole\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Partion the features from the class to predict\n",
        "df_X = df[df.columns[df.columns != 'good wine']].copy() # get columns that are not 'good wine'\n",
        "df_y = df['good wine'].copy() # get the column named 'good wine'; this is our label\n",
        "\n",
        "# (random_state): we use a fixed random seed so we get the same results every time.\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.3, random_state=1)\n",
        "\n",
        "print (\"Number of training instances: \", len(X_train), \"\\nNumber of test instances: \", len(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b988fbc-d3ec-492b-b314-46a8306d2040",
      "metadata": {
        "id": "0b988fbc-d3ec-492b-b314-46a8306d2040"
      },
      "source": [
        "Let's look at the first few rows of the training portion of the dataset.  Understanding the data is **always** important in trying to build any model for prediction.\n",
        "You can check against the description of the dataset which is here: https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality.names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "fc01a3e1-8fc8-4bec-b574-8bfbc930ffbc",
      "metadata": {
        "id": "fc01a3e1-8fc8-4bec-b574-8bfbc930ffbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "af3b1a72-3335-472d-e09d-a8927c850fd4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
              "4554            6.0              0.23         0.34             1.3      0.025   \n",
              "3401            8.8              0.19         0.30             5.0      0.028   \n",
              "3330            6.7              0.23         0.33             8.1      0.048   \n",
              "4462            7.1              0.42         0.20             2.8      0.038   \n",
              "3171            7.3              0.20         0.39             2.3      0.048   \n",
              "\n",
              "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
              "4554                 23.0                 111.0  0.98961  3.36       0.37   \n",
              "3401                 34.0                 120.0  0.99242  2.94       0.47   \n",
              "3330                 45.0                 176.0  0.99472  3.11       0.52   \n",
              "4462                 28.0                 109.0  0.98968  3.23       0.47   \n",
              "3171                 24.0                  87.0  0.99044  2.94       0.35   \n",
              "\n",
              "      alcohol  \n",
              "4554     12.7  \n",
              "3401     11.2  \n",
              "3330     10.1  \n",
              "4462     13.4  \n",
              "3171     12.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e1c3e226-7f1e-47ac-b37d-c891cf291a9c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4554</th>\n",
              "      <td>6.0</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.34</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.025</td>\n",
              "      <td>23.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>0.98961</td>\n",
              "      <td>3.36</td>\n",
              "      <td>0.37</td>\n",
              "      <td>12.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3401</th>\n",
              "      <td>8.8</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.30</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.028</td>\n",
              "      <td>34.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>0.99242</td>\n",
              "      <td>2.94</td>\n",
              "      <td>0.47</td>\n",
              "      <td>11.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3330</th>\n",
              "      <td>6.7</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.33</td>\n",
              "      <td>8.1</td>\n",
              "      <td>0.048</td>\n",
              "      <td>45.0</td>\n",
              "      <td>176.0</td>\n",
              "      <td>0.99472</td>\n",
              "      <td>3.11</td>\n",
              "      <td>0.52</td>\n",
              "      <td>10.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4462</th>\n",
              "      <td>7.1</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.20</td>\n",
              "      <td>2.8</td>\n",
              "      <td>0.038</td>\n",
              "      <td>28.0</td>\n",
              "      <td>109.0</td>\n",
              "      <td>0.98968</td>\n",
              "      <td>3.23</td>\n",
              "      <td>0.47</td>\n",
              "      <td>13.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3171</th>\n",
              "      <td>7.3</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.39</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.048</td>\n",
              "      <td>24.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>0.99044</td>\n",
              "      <td>2.94</td>\n",
              "      <td>0.35</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1c3e226-7f1e-47ac-b37d-c891cf291a9c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e1c3e226-7f1e-47ac-b37d-c891cf291a9c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e1c3e226-7f1e-47ac-b37d-c891cf291a9c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3a626e74-0c0a-4e43-8433-8c4483b27a41\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3a626e74-0c0a-4e43-8433-8c4483b27a41')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3a626e74-0c0a-4e43-8433-8c4483b27a41 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a4d9054-cb06-47e1-bf2c-22b3421a1485",
      "metadata": {
        "id": "5a4d9054-cb06-47e1-bf2c-22b3421a1485"
      },
      "source": [
        "We can also take a look at some general statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "72f10b76-0307-4e5b-9cd0-b4b2f05ef55c",
      "metadata": {
        "id": "72f10b76-0307-4e5b-9cd0-b4b2f05ef55c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "b738df8b-7497-4097-d866-f12a5ea44f21"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
              "count    3428.000000       3428.000000  3428.000000     3428.000000   \n",
              "mean        6.852728          0.278197     0.334387        6.392255   \n",
              "std         0.834514          0.101933     0.122178        5.077153   \n",
              "min         3.800000          0.080000     0.000000        0.600000   \n",
              "25%         6.300000          0.210000     0.260000        1.700000   \n",
              "50%         6.800000          0.260000     0.320000        5.200000   \n",
              "75%         7.300000          0.320000     0.390000        9.900000   \n",
              "max        11.800000          1.100000     1.230000       65.800000   \n",
              "\n",
              "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
              "count  3428.000000          3428.000000           3428.000000  3428.000000   \n",
              "mean      0.045682            35.408693            138.459160     0.994048   \n",
              "std       0.021071            17.272270             42.755363     0.003001   \n",
              "min       0.012000             3.000000             18.000000     0.987130   \n",
              "25%       0.036000            23.000000            108.000000     0.991750   \n",
              "50%       0.043000            34.000000            134.000000     0.993735   \n",
              "75%       0.050000            46.000000            168.000000     0.996120   \n",
              "max       0.346000           289.000000            440.000000     1.038980   \n",
              "\n",
              "                pH    sulphates      alcohol  \n",
              "count  3428.000000  3428.000000  3428.000000  \n",
              "mean      3.185656     0.491362    10.488486  \n",
              "std       0.151288     0.113687     1.218602  \n",
              "min       2.740000     0.230000     8.000000  \n",
              "25%       3.080000     0.410000     9.400000  \n",
              "50%       3.180000     0.480000    10.300000  \n",
              "75%       3.280000     0.550000    11.341667  \n",
              "max       3.810000     1.080000    14.200000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eb1ddede-042f-4b14-be7b-f72c318294c9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>3428.000000</td>\n",
              "      <td>3428.000000</td>\n",
              "      <td>3428.000000</td>\n",
              "      <td>3428.000000</td>\n",
              "      <td>3428.000000</td>\n",
              "      <td>3428.000000</td>\n",
              "      <td>3428.000000</td>\n",
              "      <td>3428.000000</td>\n",
              "      <td>3428.000000</td>\n",
              "      <td>3428.000000</td>\n",
              "      <td>3428.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>6.852728</td>\n",
              "      <td>0.278197</td>\n",
              "      <td>0.334387</td>\n",
              "      <td>6.392255</td>\n",
              "      <td>0.045682</td>\n",
              "      <td>35.408693</td>\n",
              "      <td>138.459160</td>\n",
              "      <td>0.994048</td>\n",
              "      <td>3.185656</td>\n",
              "      <td>0.491362</td>\n",
              "      <td>10.488486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.834514</td>\n",
              "      <td>0.101933</td>\n",
              "      <td>0.122178</td>\n",
              "      <td>5.077153</td>\n",
              "      <td>0.021071</td>\n",
              "      <td>17.272270</td>\n",
              "      <td>42.755363</td>\n",
              "      <td>0.003001</td>\n",
              "      <td>0.151288</td>\n",
              "      <td>0.113687</td>\n",
              "      <td>1.218602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>3.800000</td>\n",
              "      <td>0.080000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.012000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>0.987130</td>\n",
              "      <td>2.740000</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>8.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>6.300000</td>\n",
              "      <td>0.210000</td>\n",
              "      <td>0.260000</td>\n",
              "      <td>1.700000</td>\n",
              "      <td>0.036000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>108.000000</td>\n",
              "      <td>0.991750</td>\n",
              "      <td>3.080000</td>\n",
              "      <td>0.410000</td>\n",
              "      <td>9.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>6.800000</td>\n",
              "      <td>0.260000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>5.200000</td>\n",
              "      <td>0.043000</td>\n",
              "      <td>34.000000</td>\n",
              "      <td>134.000000</td>\n",
              "      <td>0.993735</td>\n",
              "      <td>3.180000</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>10.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>7.300000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.390000</td>\n",
              "      <td>9.900000</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>46.000000</td>\n",
              "      <td>168.000000</td>\n",
              "      <td>0.996120</td>\n",
              "      <td>3.280000</td>\n",
              "      <td>0.550000</td>\n",
              "      <td>11.341667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>11.800000</td>\n",
              "      <td>1.100000</td>\n",
              "      <td>1.230000</td>\n",
              "      <td>65.800000</td>\n",
              "      <td>0.346000</td>\n",
              "      <td>289.000000</td>\n",
              "      <td>440.000000</td>\n",
              "      <td>1.038980</td>\n",
              "      <td>3.810000</td>\n",
              "      <td>1.080000</td>\n",
              "      <td>14.200000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eb1ddede-042f-4b14-be7b-f72c318294c9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-eb1ddede-042f-4b14-be7b-f72c318294c9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-eb1ddede-042f-4b14-be7b-f72c318294c9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dcdbfa2d-1a14-4038-bbe8-746465cfa827\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dcdbfa2d-1a14-4038-bbe8-746465cfa827')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dcdbfa2d-1a14-4038-bbe8-746465cfa827 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "X_train.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad0e0a49-c2a0-490d-970a-71eeebb6ed58",
      "metadata": {
        "id": "ad0e0a49-c2a0-490d-970a-71eeebb6ed58"
      },
      "source": [
        "### .c Train and Test the models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbc9decb-8422-4740-8301-ca2bec8c4b06",
      "metadata": {
        "id": "cbc9decb-8422-4740-8301-ca2bec8c4b06"
      },
      "source": [
        "Actually all of the above was just preparation. Now we have some intution, let's try fitting the dataset on both $k$ - NN and Decision Tree models using sklearn's API library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "98efb8fa-3bca-4734-8f71-d6f37ddde246",
      "metadata": {
        "id": "98efb8fa-3bca-4734-8f71-d6f37ddde246",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49159556-3d20-4102-9d0a-b7a6561299f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kNN accuracy for training set: 1.000000\n",
            "kNN accuracy for test set: 0.794558\n"
          ]
        }
      ],
      "source": [
        "# Get the machine learning algorithm k-NN\n",
        "from sklearn import neighbors\n",
        "\n",
        "knn = neighbors.KNeighborsClassifier(n_neighbors = 1, metric='euclidean')\n",
        "knn_model = knn.fit(X_train, y_train)\n",
        "print('kNN accuracy for training set: %f' % knn_model.score(X_train, y_train))\n",
        "print('kNN accuracy for test set: %f' % knn_model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "2aa8375a-a076-49a9-b182-d43bb1247288",
      "metadata": {
        "id": "2aa8375a-a076-49a9-b182-d43bb1247288",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd004f19-c974-4166-c67c-67c62d207975"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree accuracy for training set: 1.000000\n",
            "Decision Tree accuracy for test set: 0.819048\n"
          ]
        }
      ],
      "source": [
        "# Get the machine learning algorithm Naïve Bayes\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Fix the random seed for decision tree classifier\n",
        "np.random.seed(seed=0)\n",
        "dt = DecisionTreeClassifier(max_depth=None)\n",
        "dt_model = dt.fit(X_train,y_train)\n",
        "print('Decision Tree accuracy for training set: %f' % dt_model.score(X_train, y_train))\n",
        "print('Decision Tree accuracy for test set: %f' % dt_model.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e1d46fa-3e2e-4e5a-a908-25b046f8331e",
      "metadata": {
        "id": "6e1d46fa-3e2e-4e5a-a908-25b046f8331e"
      },
      "source": [
        "After running these experiments you should have been able to get about 79% accuracy (on test set) for the nearest neighbor code and about 82% accuracy (on test set) using the Decision Tree algorithm.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CDmjJP10R4Kd",
      "metadata": {
        "id": "CDmjJP10R4Kd"
      },
      "source": [
        "That was easy! But it is important to know how to master these models yourself, so you're going to implement from scratch.  You'll practice this in the next sections."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13c6f7ac-1bcc-4e8a-919c-73650f0225f2",
      "metadata": {
        "id": "13c6f7ac-1bcc-4e8a-919c-73650f0225f2"
      },
      "source": [
        "To ease the implementation in the next section, we'll first transform the data into `numpy` arrays and transform the labels into lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "71c9452f-69c1-4d43-8d8f-729ffed05b7f",
      "metadata": {
        "id": "71c9452f-69c1-4d43-8d8f-729ffed05b7f"
      },
      "outputs": [],
      "source": [
        "X_train, X_test = X_train.to_numpy(), X_test.to_numpy()\n",
        "y_train, y_test = y_train.to_list(), y_test.to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7f64225-4087-4081-ac7d-641b4718be7d",
      "metadata": {
        "id": "c7f64225-4087-4081-ac7d-641b4718be7d"
      },
      "source": [
        "## 2. Programming : Implement $k$-Nearest Neighbor (kNN)\n",
        "\n",
        "We are going to implement a kNN to predict whether the wine is \"Good\" or \"Not good\" (so it is a binary or 2-class classification)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b20606f9-5045-43b4-ac4f-7fd1c6829823",
      "metadata": {
        "id": "b20606f9-5045-43b4-ac4f-7fd1c6829823"
      },
      "source": [
        "Next we are going to build a list of helper functions to help build the kNN model. Your task is to implement these functions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b94665f-50da-4bad-9544-36845fbc8aa3",
      "metadata": {
        "id": "3b94665f-50da-4bad-9544-36845fbc8aa3"
      },
      "source": [
        "### .a Calculate Manhattan_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fc1819f-fb15-4250-b763-b38cfa1c1221",
      "metadata": {
        "id": "2fc1819f-fb15-4250-b763-b38cfa1c1221"
      },
      "source": [
        "**Manhattan Distance**: Manhattan Distance measures the sum of the absolute differences of two vectors' Cartesian coordinates.\n",
        "\n",
        "$$\n",
        "L_1(a, b) = |a_1-b_1| + |a_2-b_2| + \\cdots + |a_n-b_n|\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9356fc39-5fb8-4b51-ad0c-e30c5c5b1aa1",
      "metadata": {
        "id": "9356fc39-5fb8-4b51-ad0c-e30c5c5b1aa1"
      },
      "source": [
        "**Your Turn (Question 1):** Complete the code below to calculate the Manhattan Distance between two data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "8c07a2c5-69f8-4f69-b1bb-205c6e1e5176",
      "metadata": {
        "id": "8c07a2c5-69f8-4f69-b1bb-205c6e1e5176"
      },
      "outputs": [],
      "source": [
        "def manhattan_distance(a, b):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      a (D) : a data point in numpy array of dimension D\n",
        "      b (D) : a data point in numpy array of dimension D\n",
        "    Returns:\n",
        "      dis(float): the Manhattan distance between the two data points\n",
        "    \"\"\"\n",
        "    dis = 0\n",
        "    ###########################\n",
        "    #\n",
        "    # Your Turn (Q1): Write your code here\n",
        "    # Hint: calculate vector c which is the element-wise absolute difference between vector a and b; find out the sum of all elements in c\n",
        "    # Hint: use the numpy library to speed up matrix and vector operations like difference, summation and square root. https://numpy.org/doc/stable/user/quickstart.html\n",
        "    #\n",
        "    ###########################\n",
        "    c = np.abs(a - b)\n",
        "    dis = np.sum(c)\n",
        "    return dis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a522637b",
      "metadata": {
        "id": "a522637b"
      },
      "source": [
        "### .b Calculate Euclidean_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "377609cc",
      "metadata": {
        "id": "377609cc"
      },
      "source": [
        "**Euclidean Distance**: Euclidean Distance measures the length of the line segment bewteen two points in the Euclidean space.\n",
        "\n",
        "$$\n",
        "L_2(a, b) = \\sqrt{(a_1-b_1)^2 + (a_2-b_2)^2 + \\cdots + (a_n-b_n)^2}\n",
        "$$\n",
        "\n",
        "Compared to the definition of Manhattan distance, it is natural to generalize the Euclidean Distance and Manhattan Distance into Minkowski Distance.\n",
        "\n",
        "**Minkowski Distance**: Minkowski Distance measures the p-order distance between two points. It is obvious that $p=1$ refers to Manhattan Distance while $p=2$ refers to Euclidean Distance.\n",
        "\n",
        "$$\n",
        "L_p(a, b) = (|a_1-b_1|^p + |a_2-b_2|^p + \\cdots + |a_n-b_n|^p)^{\\frac{1}{p}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b84f063",
      "metadata": {
        "id": "1b84f063"
      },
      "source": [
        "**Your Turn (Question 2):** Complete the code below to calculate the Minkowski Distance between two data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "8655fcb1",
      "metadata": {
        "id": "8655fcb1"
      },
      "outputs": [],
      "source": [
        "def minkowski_distance(a, b, p):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      a (D) : a data point in numpy array of dimension D\n",
        "      b (D) : a data point in numpy array of dimension D\n",
        "      p     : distance order\n",
        "    Returns:\n",
        "      dis(float): the Minkowski distance between the two data points\n",
        "    \"\"\"\n",
        "    dis = 0\n",
        "    ###########################\n",
        "    #\n",
        "    # Your Turn (Q2): Write your code here\n",
        "    # Hint: calculate vector c which is the element-wise difference between vector a and b\n",
        "    # Hint: calculate p-norm of vector c; numpy.linalg.norm() is suggested to prevent data overflow; please refer to https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html\n",
        "    #\n",
        "    ###########################\n",
        "    c = a - b\n",
        "    dis = np.linalg.norm(c, ord=p)\n",
        "    return dis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HKrVZUZEAcTx",
      "metadata": {
        "id": "HKrVZUZEAcTx"
      },
      "source": [
        "**Testing**: test to see if the Minkowski distance is consistent with your manual calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "9u-RXoP2Bx8v",
      "metadata": {
        "id": "9u-RXoP2Bx8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0486d9f-e24b-4741-f197-a831bf93a909"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minkowski Distance (p=1) between (1, 2, 3) and (0, 0, 0): 6.0\n",
            "Minkowski Distance (p=2) between (1, 2, 3) and (0, 0, 0): 3.7416573867739413\n",
            "------------------------------------------------------------------\n",
            "Minkowski Distance (p=1) between (100, 20, 30) and (0, 0, 0): 150.0\n",
            "Minkowski Distance (p=2) between (100, 20, 30) and (0, 0, 0): 106.30145812734649\n"
          ]
        }
      ],
      "source": [
        "x1 = np.asarray([1, 2, 3])\n",
        "x2 = np.asarray([0, 0, 0])\n",
        "print('Minkowski Distance (p=1) between (1, 2, 3) and (0, 0, 0):', minkowski_distance(x1, x2, p=1))\n",
        "print('Minkowski Distance (p=2) between (1, 2, 3) and (0, 0, 0):', minkowski_distance(x1, x2, p=2))\n",
        "\n",
        "print('------------------------------------------------------------------')\n",
        "x1 = np.asarray([100, 20, 30])\n",
        "x2 = np.asarray([0, 0, 0])\n",
        "print('Minkowski Distance (p=1) between (100, 20, 30) and (0, 0, 0):', minkowski_distance(x1, x2, p=1))\n",
        "print('Minkowski Distance (p=2) between (100, 20, 30) and (0, 0, 0):', minkowski_distance(x1, x2, p=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sMP2yG7DT8Gn",
      "metadata": {
        "id": "sMP2yG7DT8Gn"
      },
      "source": [
        "From the tests above, we can see that the Minkowski Distance (p=2) is closer to the dimension in which the two vectors have the largest difference, *e.g.*, $(100-0)$, $(3-0)$ than Minkowski Distance (p=1). **Why?**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43359e84",
      "metadata": {
        "id": "43359e84"
      },
      "source": [
        "**Testing**: test to see if the Minkowski distance (p=1) is consistent with Manhattan distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "b9da0eba",
      "metadata": {
        "id": "b9da0eba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3903b5a-5e80-439f-be2f-e2b9bd6cb30c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manhattan Distance between (1.5, 2.5, 3.5) and (0., 0., 0.): 7.5\n",
            "Minkowski Distance (p=1) between (1.5, 2.5, 3.5) and (0., 0., 0.): 7.5\n",
            "Manhattan Distance between (10., 20., 30.) and (13., 4., 7.): 42.0\n",
            "Minkowski Distance (p=1) between (10., 20., 30.) and (13., 4., 7.): 42.0\n"
          ]
        }
      ],
      "source": [
        "x1 = np.asarray([1.5, 2.5, 3.5])\n",
        "x2 = np.asarray([0., 0., 0.])\n",
        "print('Manhattan Distance between (1.5, 2.5, 3.5) and (0., 0., 0.):', manhattan_distance(x1, x2))\n",
        "print('Minkowski Distance (p=1) between (1.5, 2.5, 3.5) and (0., 0., 0.):', minkowski_distance(x1, x2, p=1))\n",
        "\n",
        "x1 = np.asarray([10., 20., 30.])\n",
        "x2 = np.asarray([13., 4., 7.])\n",
        "print('Manhattan Distance between (10., 20., 30.) and (13., 4., 7.):', manhattan_distance(x1, x2))\n",
        "print('Minkowski Distance (p=1) between (10., 20., 30.) and (13., 4., 7.):', minkowski_distance(x1, x2, p=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15668366",
      "metadata": {
        "id": "15668366"
      },
      "source": [
        "Since we can use minkowski_distance to calculate both Manhattan Distance and Euclidean Distance, we will keep using minkowski_distance function instead of manhattan_distance in the following programming.\n",
        "\n",
        "**Optional**: test to see what happened when p > 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "CmfD_TS4A6a3",
      "metadata": {
        "id": "CmfD_TS4A6a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a68acc16-bc8f-431d-e15d-0db8ac487c4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "84.44345780422779\n",
            "99.69812000000002\n"
          ]
        }
      ],
      "source": [
        "# Testing: This should return 84.44345780422779\n",
        "print(minkowski_distance(X_test[0], X_train[0], p=2))\n",
        "\n",
        "# Testing: This should return 99.69812000000002\n",
        "print(minkowski_distance(X_test[0], X_train[0], p=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec1e5566-f7bd-4ba7-9882-932fc27e26d7",
      "metadata": {
        "id": "ec1e5566-f7bd-4ba7-9882-932fc27e26d7"
      },
      "source": [
        "### .c Find k Nearest Neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13eea814-4b5c-4437-bd63-27b508fcc89e",
      "metadata": {
        "id": "13eea814-4b5c-4437-bd63-27b508fcc89e"
      },
      "source": [
        "**$k$-NN function**: The $k$-NN function will find the k nearest data points given an array of distances. We want to return the corresponding labels of the k Nearest Neighbors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d41ac3ab-2523-4992-9f29-fa8e6b18610c",
      "metadata": {
        "id": "d41ac3ab-2523-4992-9f29-fa8e6b18610c"
      },
      "source": [
        "**Your Turn (Question 3):** Complete the code below to find out the kNN's labels with the given array of distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "603a33b7-431f-4b37-b685-08e5556e578b",
      "metadata": {
        "id": "603a33b7-431f-4b37-b685-08e5556e578b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba6ed007-23fb-45b9-e02e-07c180438cda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 1, 0, 1, 2]\n"
          ]
        }
      ],
      "source": [
        "def find_kNN_labels(distances, labels, k):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      distances (m,) : a numpy array of dimension m that contains the distances between the test data point and all training data points\n",
        "      labels (m,) : a list of length m that contains the labels of all training data points\n",
        "      k: the number of nearest neighbors\n",
        "    Returns:\n",
        "      knn_labels (k,): the labels of the k nearest neighbors\n",
        "    \"\"\"\n",
        "\n",
        "    knn_labels = []\n",
        "    ###########################\n",
        "    #\n",
        "    # Your Turn (Q3): Write your code here\n",
        "    # Hint: use numpy.argsort function to sort distances array and obtain the sorted indice; select the k indice with the least distances; return the corresponding labels of the selected k indice\n",
        "    # Hint: use \"slicing\", e.g., list_of_names[:k], to select the first k elements in python list and numpy array. https://stackoverflow.com/questions/509211/understanding-slice-notation\n",
        "    #\n",
        "    ###########################\n",
        "\n",
        "    # get the distances\n",
        "    sorted_indices = np.argsort(distances)\n",
        "    # Select the indices of the k smallest distances\n",
        "    k_smallest_indices = sorted_indices[:k]\n",
        "    knn_labels = [labels[i] for i in k_smallest_indices]\n",
        "\n",
        "    return knn_labels\n",
        "\n",
        "# Testing: This should return [2, 1, 0, 1, 2]\n",
        "sample_distances = [1.97, 1.94, 0.16, 0.91, 2.05, 1.5, 1.86, 2.01, 1.9, 1.67, 1.9, 1.14, 2.1, 1.94, 2.68, 0.08, 3.98, 3.05, 1.59, 2.4]\n",
        "sample_labels = [0, 0, 1, 0, 1, 2, 1, 2, 1, 0, 1, 1, 0, 2, 1, 2, 0, 2, 2, 1]\n",
        "print(find_kNN_labels(sample_distances, sample_labels, 5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2FbdUK94lZpp",
      "metadata": {
        "id": "2FbdUK94lZpp"
      },
      "source": [
        "### .d Find the Majority Class"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1P6SRD1Mmudn",
      "metadata": {
        "id": "1P6SRD1Mmudn"
      },
      "source": [
        "**Majority Class Function**: When finding the $k$-Nearest Neighbors, we return the value that represents the majority of the $k$ instances of the class as the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Cs6Px7tkl-ud",
      "metadata": {
        "id": "Cs6Px7tkl-ud"
      },
      "source": [
        "**Your Turn (Question 4):** Complete the code below to find the majority class from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "ri7bAZaQlXTV",
      "metadata": {
        "id": "ri7bAZaQlXTV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3661dd6e-5b9b-4180-c4fc-34092451544c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good\n"
          ]
        }
      ],
      "source": [
        "def get_majority_class(labels):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      labels(m): The corresponding labels of current sub-dataset\n",
        "          m: num_rows\n",
        "    Returns:\n",
        "      major: Type:String. The major class of this sub-dataset(e.g \"Good Wine\" or \"Not Good\")\n",
        "    \"\"\"\n",
        "    major = \"\"\n",
        "\n",
        "    # freq will store the number of occurences of the target labels\n",
        "    freq = {}\n",
        "    for entry in labels:\n",
        "        if (entry in freq):\n",
        "            freq[entry] += 1.0\n",
        "        else:\n",
        "            freq[entry] = 1.0\n",
        "\n",
        "    major = \"\"\n",
        "    ###########################\n",
        "    #\n",
        "    # Your Turn (Q4): Write your code here\n",
        "    # Hint: Loop through each entry in labels, then find which entry occurs most frequently.\n",
        "    #\n",
        "    #\n",
        "    ###########################\n",
        "\n",
        "    # freq will store the number of occurrences of the target labels\n",
        "    freq = {}\n",
        "    for entry in labels:\n",
        "        if entry in freq:\n",
        "            freq[entry] += 1\n",
        "        else:\n",
        "            freq[entry] = 1\n",
        "\n",
        "    # Find the label with the maximum frequency\n",
        "    major = max(freq, key=freq.get)\n",
        "    return major\n",
        "\n",
        "\n",
        "# Testing: This should return 'Good'\n",
        "sample_y = y_train[50:56]\n",
        "print(get_majority_class(sample_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "366e998f-51cf-43a9-a33a-bec29c01913a",
      "metadata": {
        "id": "366e998f-51cf-43a9-a33a-bec29c01913a"
      },
      "source": [
        "### .e Run $k$-NN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69143bc8-f0ba-41b5-ae1f-341746ef9910",
      "metadata": {
        "id": "69143bc8-f0ba-41b5-ae1f-341746ef9910"
      },
      "source": [
        "The following code `run_knn` is provided to you to run your kNN helper functions.\n",
        "\n",
        "You do not have to understand its code for the purpose of this exercise. Just run it and check the accuracy.\n",
        "\n",
        "Note that because the prediction of a single data points requires traversing the whole training set. The code is a bit slow. It can take $2\\sim 4$ minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "bc244881-0caa-41c1-a237-e6ebf10e93ff",
      "metadata": {
        "id": "bc244881-0caa-41c1-a237-e6ebf10e93ff"
      },
      "outputs": [],
      "source": [
        "def run_knn(distance_metric_function, **kwags):\n",
        "    k = 1\n",
        "    correct = 0\n",
        "    for test_entry, label in zip(X_test, y_test):\n",
        "\n",
        "        ## find out the distance between the test data point and all training data points\n",
        "        distances = []\n",
        "        for train_entry in X_train:\n",
        "            distances.append(distance_metric_function(test_entry, train_entry, **kwargs))\n",
        "\n",
        "        knn_labels = find_kNN_labels(distances, y_train, k)\n",
        "        prediction = get_majority_class(knn_labels)\n",
        "\n",
        "        if prediction == label:\n",
        "            correct += 1\n",
        "\n",
        "    accuracy = correct / len(y_test)\n",
        "    print('Final accuracy')\n",
        "    print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cbc9fcf-ee0b-4654-b3a6-4cc41021130f",
      "metadata": {
        "id": "8cbc9fcf-ee0b-4654-b3a6-4cc41021130f"
      },
      "source": [
        "The following code runs $k$-NN with the Euclidean distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "a069a328-adb9-42ea-bd3b-3499ea0a2132",
      "metadata": {
        "id": "a069a328-adb9-42ea-bd3b-3499ea0a2132",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "000767f8-2cec-41e3-94da-f02f54641256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final accuracy\n",
            "0.7945578231292517\n"
          ]
        }
      ],
      "source": [
        "## The accuracy should be 0.7945578231292517.\n",
        "kwargs = {}\n",
        "kwargs['p'] = 2\n",
        "run_knn(minkowski_distance, **kwargs) # note that minkowski_distance is a function, not a value"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3a2c83e-95a2-42a4-933f-311d524a80ac",
      "metadata": {
        "id": "f3a2c83e-95a2-42a4-933f-311d524a80ac"
      },
      "source": [
        "The following code runs $k$-NN with the Manhattan distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "9d81df58",
      "metadata": {
        "id": "9d81df58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "651f5121-24b9-4bcc-acf9-91116f66bfec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final accuracy\n",
            "0.8020408163265306\n"
          ]
        }
      ],
      "source": [
        "## The accuracy should be 0.8020408163265306\n",
        "kwargs = {}\n",
        "kwargs['p'] = 1\n",
        "run_knn(minkowski_distance, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae64e6d7",
      "metadata": {
        "id": "ae64e6d7"
      },
      "source": [
        "**Optional**: The following code runs $k$-NN with the Minkowski distance (p=10)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "858f4d42-9a0c-448b-8440-b8c3c8927853",
      "metadata": {
        "id": "858f4d42-9a0c-448b-8440-b8c3c8927853",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bea94f47-1a58-4383-a7a8-850763535030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final accuracy\n",
            "0.7925170068027211\n"
          ]
        }
      ],
      "source": [
        "## The accuracy should be 0.7925170068027211\n",
        "kwargs = {}\n",
        "kwargs['p'] = 10\n",
        "run_knn(minkowski_distance, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0048bf28-85ef-4f17-91df-e5bd34fd7ab5",
      "metadata": {
        "id": "0048bf28-85ef-4f17-91df-e5bd34fd7ab5"
      },
      "source": [
        "## 3. Programming : Implement Decision Tree\n",
        "\n",
        "We are going to implement a decision tree to predict whether the wine is \"Good\" or \"Not good\" (again, it is a 2-class classification)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f497ee5d-968f-4554-b9d7-e1307c990ec3",
      "metadata": {
        "id": "f497ee5d-968f-4554-b9d7-e1307c990ec3"
      },
      "source": [
        "### .a Discretize the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cee51a3-b13f-4f64-a479-2202cbea65b8",
      "metadata": {
        "id": "5cee51a3-b13f-4f64-a479-2202cbea65b8"
      },
      "source": [
        "We are going to make use of 11 _attributes_ (also known as _features_ or _input dimensions_, but for this decision trees to be consistent with the algorithm, we'll use \"attributes\").\n",
        "\n",
        "In this assignment, we will implement a basic version of Decision tree that takes in categorical attributes. Thus, we are going to discretize the continuous features, where values **larger** than the mean is assigned **1** and values **smaller** than the mean is assigned **0**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "52c8a15a-091e-40e7-b85f-61a8e79e052e",
      "metadata": {
        "id": "52c8a15a-091e-40e7-b85f-61a8e79e052e"
      },
      "outputs": [],
      "source": [
        "def discretize_data(train_data, test_data):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        train_data (np array): the training set\n",
        "        test_data (np array): the test set\n",
        "\n",
        "    Returns:\n",
        "    np_train_data (np array): contains the discretized training set\n",
        "    np_test_data (np array): contains the discretized test set\n",
        "    \"\"\"\n",
        "    train_data_discrete = np.zeros_like(train_data) # initialize\n",
        "    test_data_discrete = np.zeros_like(test_data)\n",
        "\n",
        "    D = train_data_discrete.shape[1]\n",
        "\n",
        "    for i in range(D):\n",
        "        mean_value = np.mean(train_data[:, i]) # get mean values at which to label as 1 (larger) or 0 (smaller)\n",
        "        train_data_discrete[:, i] = (train_data[:, i] > mean_value).astype(float)\n",
        "        test_data_discrete[:, i] = (test_data[:, i] > mean_value).astype(float)\n",
        "\n",
        "    return train_data_discrete, test_data_discrete\n",
        "\n",
        "X_train_discrete, X_test_discrete = discretize_data(X_train, X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "06203550-afe2-4496-b0b8-53da94f6b5c1",
      "metadata": {
        "id": "06203550-afe2-4496-b0b8-53da94f6b5c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84251feb-e2f9-41df-94a6-1698118224e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.],\n",
              "        [0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
              "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "        [1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.],\n",
              "        [0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "        [1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1.],\n",
              "        [0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1.],\n",
              "        [0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1.],\n",
              "        [1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0.]]),\n",
              " ['Not Good',\n",
              "  'Good',\n",
              "  'Not Good',\n",
              "  'Not Good',\n",
              "  'Not Good',\n",
              "  'Not Good',\n",
              "  'Good',\n",
              "  'Good',\n",
              "  'Good',\n",
              "  'Not Good'])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "sample_X = X_train_discrete[10:20]\n",
        "sample_y = y_train[10:20]\n",
        "sample_X, sample_y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "193e7af2-57e0-494a-b1fe-ce16348f05ea",
      "metadata": {
        "id": "193e7af2-57e0-494a-b1fe-ce16348f05ea"
      },
      "source": [
        "### .b Choose the best Feature based on Majority"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08aa95d2-a41a-4288-9cbc-f5fd38afccc1",
      "metadata": {
        "id": "08aa95d2-a41a-4288-9cbc-f5fd38afccc1"
      },
      "source": [
        "**Majority Class Function**: When you reach a terminal node (leaf) in a decision tree, we return the label that represents the majority of the sub-dataset's instances of the class as the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Bv2g3fB_mCQS",
      "metadata": {
        "id": "Bv2g3fB_mCQS"
      },
      "source": [
        "We can reuse the `get_majority_class` function from the implementation of $k$-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "732bc81a-6c81-4d82-94a1-f64ac5ca4eed",
      "metadata": {
        "id": "732bc81a-6c81-4d82-94a1-f64ac5ca4eed"
      },
      "source": [
        "### .c Calculate Entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bd7f40c-8263-429c-b2ad-71d0d14dc61f",
      "metadata": {
        "id": "5bd7f40c-8263-429c-b2ad-71d0d14dc61f"
      },
      "source": [
        "**Entropy Function**: Calculate the entropy of this current sub-dataset:\n",
        "\n",
        "Recall: $H(X) = - \\sum_{i=0}^C p_i\\log_2(p_i)$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f95fbba7-c7eb-4a60-a123-38f109edb54b",
      "metadata": {
        "id": "f95fbba7-c7eb-4a60-a123-38f109edb54b"
      },
      "source": [
        "**Your Turn (Question 5):** Complete the code below to calculate entropy of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "3426b56e-ab12-416f-b9b0-dd91099d828e",
      "metadata": {
        "id": "3426b56e-ab12-416f-b9b0-dd91099d828e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2cff0b0-0fc5-41d2-a81f-92c39d870b33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9709505944546686\n"
          ]
        }
      ],
      "source": [
        "def entropy(labels):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      labels(m): The corresponding labels of current sub-dataset of the sub-tree\n",
        "          m: num_rows\n",
        "    Returns:\n",
        "      dataEntropy: The entropy of this current sub-dataset\n",
        "    \"\"\"\n",
        "    dataEntropy = 0.0\n",
        "    # freq will store the number of occurrences of the target labels\n",
        "    freq = {}\n",
        "    for entry in labels:\n",
        "        if (entry in freq):\n",
        "            freq[entry] += 1.0\n",
        "        else:\n",
        "            freq[entry] = 1.0\n",
        "\n",
        "    ###########################\n",
        "    #\n",
        "    # Your Turn (Q5): Write your code here\n",
        "    #\n",
        "    ###########################\n",
        "\n",
        "    total = sum(freq.values())\n",
        "    for entry in freq:\n",
        "        probability = freq[entry] / total\n",
        "        dataEntropy -= probability * math.log2(probability)\n",
        "\n",
        "    return dataEntropy\n",
        "\n",
        "# Testing: This should return 0.9709505944546686 (in log 2)\n",
        "print(entropy(sample_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43dcec20-82f1-4b7b-9500-313b9bbe8bc2",
      "metadata": {
        "id": "43dcec20-82f1-4b7b-9500-313b9bbe8bc2"
      },
      "source": [
        "### .d Calculate Information Gain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3c82aa8-3f79-4b57-a111-4ef1969eee7b",
      "metadata": {
        "id": "c3c82aa8-3f79-4b57-a111-4ef1969eee7b"
      },
      "source": [
        "**Information Gain**: The information gained by splitting the current (sub)-dataset using the attribute.\n",
        "\n",
        "Recall:  Information Gain is a metric that measures the expected reduction in the impurity of the collection $S$, caused by splitting the data according to any given attribute. A chosen attribute $x_i$ divides the example set S into subsets\n",
        "$S_1 , S_2 , ... , S_{C_i}$ according to the $C_i$ distinct values for $x_i$ .\n",
        "The entropy then reduces to the entropy of the subsets $S_1 , S_2 , ... , S_{C_i}$:\n",
        "\n",
        "<div align=\"center\">\n",
        "$\\text{remainder}(S, x_i) = \\sum_{j=1}^{C_i} \\frac{|S_j|}{|S|} H(S_j)$\n",
        "</div>\n",
        "\n",
        "The Information Gain (IG; “reduction in entropy”) from knowing the value of $x_i$ is:\n",
        "<div align=\"center\">\n",
        "$IG(S, x_i) = H(S) - \\text{remainder}(S, x_i) $  \n",
        "</div>\n",
        "\n",
        "Subsequently, we choose the attribute with the largest IG."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8541937a-d959-4e6d-84bc-9610b83daa66",
      "metadata": {
        "id": "8541937a-d959-4e6d-84bc-9610b83daa66"
      },
      "source": [
        "**Your Turn (Question 6):** Complete the code below to calculate information gain of a given attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "de08aa3c-16fa-498f-8b16-57d571126ad3",
      "metadata": {
        "id": "de08aa3c-16fa-498f-8b16-57d571126ad3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e452e324-8d97-4d3f-9e60-2a05dfa9a832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.01997309402197489\n"
          ]
        }
      ],
      "source": [
        "def info_gain(data, labels, attribute, attributes):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      data(m, D): The current sub-dataset of the sub-tree\n",
        "          m: num_rows\n",
        "          D: num_features\n",
        "      labels(m): The corresponding labels of current sub-dataset of the sub-tree\n",
        "          m: num_rows\n",
        "      attribute: The attribute used to split data\n",
        "      attributes: The list of current remaining attributes\n",
        "    Returns:\n",
        "      info_gain : information gain of the given dataset\n",
        "    \"\"\"\n",
        "\n",
        "    freq = {}\n",
        "    subsetEntropy = 0.0\n",
        "\n",
        "    # Get the column index of this attribute\n",
        "    i = attributes.index(attribute)\n",
        "\n",
        "    for entry in data:\n",
        "        if (entry[i] in freq):\n",
        "            freq[entry[i]] += 1.0\n",
        "        else:\n",
        "            freq[entry[i]]  = 1.0\n",
        "\n",
        "    ###########################\n",
        "    #\n",
        "    # Your Turn (Q6): Write your code here\n",
        "    # Hint: Split the data based on the value at index i. Find the subsetEntropy of\n",
        "    # each sub-dataset, then use the formula of Information Gain to calculate subsetEntropy.\n",
        "    #\n",
        "    ###########################\n",
        "    total_entropy = entropy(labels)\n",
        "    freq = {}\n",
        "    subsetEntropy = 0.0\n",
        "\n",
        "    # Get the column index of this attribute\n",
        "    i = attributes.index(attribute)\n",
        "\n",
        "    # Splitting the data based on the attribute and counting occurrences\n",
        "    for index, entry in enumerate(data):\n",
        "        key = entry[i]\n",
        "        if key not in freq:\n",
        "            freq[key] = []\n",
        "        freq[key].append(labels[index])\n",
        "\n",
        "    # Calculating the entropy for each subset and their weighted sum\n",
        "    for key in freq:\n",
        "        subset_labels = freq[key]\n",
        "        subset_prob = len(subset_labels) / float(len(data))\n",
        "        subset_entropy = entropy(subset_labels)\n",
        "        subsetEntropy += subset_prob * subset_entropy\n",
        "\n",
        "    return (entropy(labels) - subsetEntropy)\n",
        "\n",
        "# Testing: This should return 0.01997309402197489\n",
        "attributes = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
        "print(info_gain(sample_X, sample_y, 'residual sugar', attributes))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49490e04-d33c-4f72-a6d4-dadcf2dcaf5b",
      "metadata": {
        "id": "49490e04-d33c-4f72-a6d4-dadcf2dcaf5b"
      },
      "source": [
        "### .e Find the Best Attribute"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "156d86b5-6fa4-43a1-a7e2-22c74a98d418",
      "metadata": {
        "id": "156d86b5-6fa4-43a1-a7e2-22c74a98d418"
      },
      "source": [
        "Now we will write a function to choose the **best** (most discriminating) attribute for a given data, here best indicates having **largest** information gain (IG) among all attributes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9b03b21-a83c-4b15-9eec-261c552af288",
      "metadata": {
        "id": "f9b03b21-a83c-4b15-9eec-261c552af288"
      },
      "source": [
        "**Your Turn (Question 7):** Complete the code below to get the best attribute based on information gain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "5fb2edb1-21b2-44bd-b3c8-cc700bfb6e8a",
      "metadata": {
        "id": "5fb2edb1-21b2-44bd-b3c8-cc700bfb6e8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c5ec81a-5ad6-45c9-fe2d-31599ad7002d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "citric acid\n"
          ]
        }
      ],
      "source": [
        "def get_best_gain_attribute(data, labels, attributes):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      data(m, D): The current sub-dataset of the sub-tree\n",
        "          m: num_rows\n",
        "          D: num_features + 1 (last column is the label)\n",
        "      attributes: The list of current remaining attributes\n",
        "    Returns:\n",
        "      best: The best attribute to split based on info gain.\n",
        "    \"\"\"\n",
        "    best = attributes[0]\n",
        "\n",
        "    max_gain = 0\n",
        "    for attr in attributes:\n",
        "      ###########################\n",
        "      #\n",
        "      # Your Turn (Q7): Write your code here\n",
        "      # Hint: For each attribute in attributes, use info_gain function above\n",
        "      # to know which attribute is the best option\n",
        "      for attr in attributes:\n",
        "        # Calculate the information gain for each attribute\n",
        "        current_gain = info_gain(data, labels, attr, attributes)\n",
        "        # Update the best attribute if current_gain is higher than max_gain\n",
        "        if current_gain > max_gain:\n",
        "            max_gain = current_gain\n",
        "            best = attr\n",
        "      pass\n",
        "      ###########################\n",
        "    return best\n",
        "\n",
        "# Testing: This should return 'citric acid'\n",
        "attributes = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
        "print(get_best_gain_attribute(sample_X, sample_y, attributes))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6aab711-569d-4ace-849c-0684a1e420f7",
      "metadata": {
        "id": "b6aab711-569d-4ace-849c-0684a1e420f7"
      },
      "source": [
        "### .f Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f15bcceb-dbe0-4f2e-99dc-c1da03572196",
      "metadata": {
        "id": "f15bcceb-dbe0-4f2e-99dc-c1da03572196"
      },
      "source": [
        "We will define two helper functions here. First, `get_unique_values` returns the unique values of a given attribute. The second function, `get_sub_data` returns the subset of rows (instances) containing a specific value of a chosen attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "2fc3c4a5-35ce-4c0b-893c-08171dd8ffe5",
      "metadata": {
        "id": "2fc3c4a5-35ce-4c0b-893c-08171dd8ffe5"
      },
      "outputs": [],
      "source": [
        "# These two functions are helper functions\n",
        "# This function will get unique values for that particular attribute from the given data\n",
        "def get_unique_values(data, attributes, attribute):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      data (m,D) : Current subset of data\n",
        "      attributes : The list of current remaining attributes\n",
        "      attribute : Our point of interest\n",
        "\n",
        "    Returns:\n",
        "      values : List of unique values for our point of interest\n",
        "    \"\"\"\n",
        "    index = attributes.index(attribute)\n",
        "    values = []\n",
        "    #\n",
        "    for entry in data:\n",
        "        if entry[index] not in values:\n",
        "            values.append(entry[index])\n",
        "\n",
        "    return values\n",
        "\n",
        "# This function will get all the rows of the data where the chosen \"best\" attribute has a value \"val\"\n",
        "def get_sub_data(data, labels, attributes, best, val):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      data (m,D) : Current subset of data\n",
        "      labels (m) : Corresponding labels of current subset of data\n",
        "      attributes : The list of current remaining attributes\n",
        "      best : The attribute of which data we will extract\n",
        "      val : We are interested only on this value of the `best` attribute\n",
        "    Returns:\n",
        "      new_data : Data subset containing only those rows where `best` attribute = val\n",
        "      new_labels : Label subset containing only those values where `best` attribute = val\n",
        "    \"\"\"\n",
        "    new_data = [[]]\n",
        "    new_labels = []\n",
        "    attribute_index = attributes.index(best)\n",
        "\n",
        "    for index, entry in enumerate(data):\n",
        "        if (entry[attribute_index] == val):\n",
        "            newEntry = []\n",
        "            for i in range(0,len(entry)):\n",
        "                if(i != attribute_index):\n",
        "                    newEntry.append(entry[i])\n",
        "            new_data.append(newEntry)\n",
        "            new_labels.append(labels[index])\n",
        "    new_data.remove([])\n",
        "    return new_data, new_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34bea7f2-dc88-4ef9-ada8-c0225dd3fa00",
      "metadata": {
        "id": "34bea7f2-dc88-4ef9-ada8-c0225dd3fa00"
      },
      "source": [
        "### .g Build the Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a716217b-4105-4fb4-ab5f-40666d1d671e",
      "metadata": {
        "id": "a716217b-4105-4fb4-ab5f-40666d1d671e"
      },
      "source": [
        "In the below code, your task is to build a tree recursively. Starting at the root, pick the best attribute to split on, and call the `build_tree` function on each of the sub-trees.  Check the inlined code comments for clarification."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44185b20-9ad4-4afd-8a83-b63bd96d6e4a",
      "metadata": {
        "id": "44185b20-9ad4-4afd-8a83-b63bd96d6e4a"
      },
      "source": [
        "**Your Turn (Question 8):** Complete the code below to build the tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "c5327d2e-0c60-41a1-a9f0-5c9870e50973",
      "metadata": {
        "id": "c5327d2e-0c60-41a1-a9f0-5c9870e50973"
      },
      "outputs": [],
      "source": [
        "def build_tree(data, labels, attributes, default):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "        data(m, D): The current sub-dataset of the sub-tree\n",
        "            m: num_rows\n",
        "            D: num_features + 1 (last column is the label)\n",
        "        attributes: The list of current remaining attributes\n",
        "      Returns:\n",
        "        tree: The constructed tree as object. For example if the root is gender,\n",
        "              then a tree of depth 2 is like\n",
        "              {'gender': {'male': sub_tree1, 'female': sub_tree2}}\n",
        "    \"\"\"\n",
        "    data = data[:]\n",
        "    tree = {}\n",
        "    ##################################################################\n",
        "    ## Your Turn (Q8): Finish the implementation of the below code ##\n",
        "    ##################################################################\n",
        "    if len(data) == 0 or len(attributes) == 0:\n",
        "        return default\n",
        "\n",
        "    # Base case: If all labels are the same, return that label\n",
        "    if all(label == labels[0] for label in labels):\n",
        "        return labels[0]\n",
        "\n",
        "    # Choose the best attribute to split the data\n",
        "    best_attribute = get_best_gain_attribute(data, labels, attributes)\n",
        "    tree = {best_attribute: {}}\n",
        "\n",
        "    # Get unique values for the best attribute\n",
        "    attribute_index = attributes.index(best_attribute)\n",
        "    unique_values = set(row[attribute_index] for row in data)\n",
        "\n",
        "    for value in unique_values:\n",
        "        # Create subsets of data and labels based on the attribute value\n",
        "        subset_data = [row for row in data if row[attribute_index] == value]\n",
        "        subset_labels = [labels[i] for i, row in enumerate(data) if row[attribute_index] == value]\n",
        "\n",
        "        # Determine the default label for the next level\n",
        "        default_label = max(set(labels), key=labels.count)\n",
        "\n",
        "        # Recursively build the subtree\n",
        "        if subset_data:\n",
        "            subtree = build_tree(subset_data, subset_labels, [attr for attr in attributes if attr != best_attribute], default_label)\n",
        "        else:\n",
        "            subtree = default_label\n",
        "\n",
        "        # Add the subtree to the tree\n",
        "        tree[best_attribute][value] = subtree\n",
        "\n",
        "    return tree"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac247c4f-5622-45da-8fb4-b48f1b9e95d2",
      "metadata": {
        "id": "ac247c4f-5622-45da-8fb4-b48f1b9e95d2"
      },
      "source": [
        "The below code block containing `run_decision_tree` does exactly as its name, and you don't have to understand its code for the purpose of this exercise. Just run it and check the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "16a807fb-aebf-4b14-8d49-3b629878e847",
      "metadata": {
        "id": "16a807fb-aebf-4b14-8d49-3b629878e847"
      },
      "outputs": [],
      "source": [
        "# Class Node which will be used while classifying a test instance using the tree built (\"fit\") earlier\n",
        "class Node():\n",
        "    value = \"\"\n",
        "    children = []\n",
        "\n",
        "    def __init__(self, val, dictionary):\n",
        "        self.value = val\n",
        "        if (isinstance(dictionary, dict)):\n",
        "            self.children = list(dictionary.keys())\n",
        "\n",
        "def run_decision_tree():\n",
        "    attributes = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
        "    tree = build_tree(X_train_discrete, y_train, attributes, get_majority_class(y_train))\n",
        "    results = []\n",
        "\n",
        "    for entry, label in zip(X_test_discrete, y_test):\n",
        "        tempDict = tree.copy()\n",
        "        result = \"\"\n",
        "        while(isinstance(tempDict, dict)):\n",
        "            root = Node(list(tempDict.keys())[0], tempDict[list(tempDict.keys())[0]])\n",
        "            tempDict = tempDict[list(tempDict.keys())[0]]\n",
        "            index = attributes.index(root.value)\n",
        "            value = entry[index]\n",
        "            if(value in list(tempDict.keys())):\n",
        "                child = Node(value, tempDict[value])\n",
        "                result = tempDict[value]\n",
        "                tempDict = tempDict[value]\n",
        "            else:\n",
        "                result = \"Null\"\n",
        "                break\n",
        "        if result != \"Null\":\n",
        "            results.append(result == label)\n",
        "\n",
        "    accuracy = float(results.count(True))/float(len(results))\n",
        "#     print(results)\n",
        "    print(\"FINAL ACCURACY: \")\n",
        "    print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "25076d80-85cc-47bb-b63f-c8956de81e4b",
      "metadata": {
        "id": "25076d80-85cc-47bb-b63f-c8956de81e4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c310e598-c9c4-4ae3-d64e-a96cc1a79b2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FINAL ACCURACY: \n",
            "0.8247787610619469\n"
          ]
        }
      ],
      "source": [
        "run_decision_tree()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e574952-8958-4b4a-b1d9-65233d061840",
      "metadata": {
        "id": "4e574952-8958-4b4a-b1d9-65233d061840"
      },
      "source": [
        "## 4. Comparison between $k$-NN and Decision Tree"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FQ85usDdFUbI",
      "metadata": {
        "id": "FQ85usDdFUbI"
      },
      "source": [
        "The above is akin to a skeleton of a machine learning project.  Critical for understanding is knowing why the performance of a learner is as it is. As a scientist, you'll want to know why you think a learner performs well or poorly and **then** use experiments to verify your hunches.  To build up this skill we need to practice it, as you'll need to apply this in your own group projects later."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b610577-3690-403c-b18f-93214458bda2",
      "metadata": {
        "id": "7b610577-3690-403c-b18f-93214458bda2"
      },
      "source": [
        "**Your Turn (Question 9):** Compare the performances of our implementation of $k$-NN and decision tree. Tell us what you have observed and why one of the models performs better than the other."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gBNkBYnOo3ks",
      "metadata": {
        "id": "gBNkBYnOo3ks"
      },
      "source": [
        "**Your answer here (Question 9)**: _Write down your answer in the current cell.  A brief 1–3 sentence answer is sufficient._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VGlAWahG2MW9",
      "metadata": {
        "id": "VGlAWahG2MW9"
      },
      "source": [
        "**Your Turn (Question 10):** Compare the running times of $k$-NN and decision tree. Briefly tell us why $k$-NN is much slower than the other."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gYlhuN-vpBTy",
      "metadata": {
        "id": "gYlhuN-vpBTy"
      },
      "source": [
        "**Your answer here (Question 10)**: _Write down your answer in the current cell.  A brief 1–3 sentence answer is sufficient._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fSVen-P92HPP",
      "metadata": {
        "id": "fSVen-P92HPP"
      },
      "source": [
        "**Your Turn (Question 11):** Compare the performances of your implementation of decision tree and sklearn's implementation. Which one has the better performance? Why do you think that is?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5W8zvwKapCvJ",
      "metadata": {
        "id": "5W8zvwKapCvJ"
      },
      "source": [
        "**Your answer here (Question 11)**: _Write down your answer in the current cell.  A brief 1–3 sentence answer is sufficient._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h0ySjsCG3r9j",
      "metadata": {
        "id": "h0ySjsCG3r9j"
      },
      "source": [
        "**Your Turn (Question 12):** Identify an instance where $k$-NN predicts a different label than Decision Tree. Briefly tell us why you think they predicted differently, and what can you do to make them agree more?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Z3c_dfKQ3sAy",
      "metadata": {
        "id": "Z3c_dfKQ3sAy"
      },
      "source": [
        "**Your answer here (Question 12)**: _Write down your answer in the current cell.  A brief 1–3 sentence answer is sufficient._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j5yziGEN4J1U",
      "metadata": {
        "id": "j5yziGEN4J1U"
      },
      "source": [
        "Congratulations 🎉 🎉 ! You have come to the end of the assignment.\n",
        "**Remember** to submit your \"Your Turn\" code, results, and answers to Canvas to be graded for your work."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4e574952-8958-4b4a-b1d9-65233d061840"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}